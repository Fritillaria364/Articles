{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document-level Neural Machine Translation with Hierarchcal Attention Network\n",
    "Lesly Miculicich et.al. EMNLP2018  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## どんなもの\n",
    "動的に文脈を捉えるための階層的アテンションモデルをNMT構造に導入  \n",
    "モデルはNMTの以前の隠れ状態に基づいた別レベルの抽象を統合する  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 先行研究との比較\n",
    "NMTでは、文と文の繋がりや文脈情報を考慮しない簡素な仕組みであるため、翻訳文における一貫性が低下している  \n",
    "最近の研究ではNMTモデルに文脈情報を追加して一般的な翻訳の性能を向上させる試みが行われており、翻訳文における一貫性・結束性が改善されている  \n",
    "ほとんどの手法は追加のエンコーダーを使って以前のソース文から文脈情報を抽出する手法が取られている  \n",
    "しかしこの手法だと追加のパラメータが必要になる上に既存のNMTによって学習された表現を用いない  \n",
    "より近年ではキャッシュベースのメモリネットワークを用いた手法がエンコーダベースドよりもよい性能を示している  \n",
    "\n",
    "> ### キャッシュベースメモリネットワーク\n",
    "> 過去の文脈を単語のセットとして保存し、NMTが翻訳時に学習した隠れ状態を保持する一つのユニークな単語が各セルと対応する？？？？？\n",
    "\n",
    "しかしこの手法だと単語表現がそれらが表れる文とは無関係に保存され、分散表現が元のNMTモデルからかけ離れたものになってしまう？？？？？  \n",
    "(大事なことは、document-level translationにおける有名なアプローチは追加のエンコーダやメモリネットワークを用いる手法であるということ)  \n",
    "\n",
    "### 具体的な手法\n",
    "筆者が提案する手法は階層的アテンションネットワーク(HAN)を使って、単語レベル・文レベルで文脈情報を体系的にモデリングする  \n",
    "階層的RNNとは対称的に、このアテンションは異なる文・各推測単語に対して選択的に焦点を当てることで文脈に対する動的なアクセスを可能としている  \n",
    "加えて提案手法ではターゲット側とソース側で２つのHANsをNMTモデルに導入する  \n",
    "HANエンコーダはソース単語の曖昧性回避に寄与し、一方でHANデコーダはターゲット側の語彙一貫性・結合性に寄与する  \n",
    "HANsの結合は(i)エンコーダ/デコーダ両方における過去の翻訳の隠れ状態の再利用、(ii)現在の翻訳におけるエンコーダ/デコーダ両方への入力によりなされる  \n",
    "この結合手法により複数文の結合の最適化を可能とする  \n",
    "\n",
    "### ここだけ見りゃいい  \n",
    "本論文の主論  \n",
    "(i) 文脈や文の潜在的な繋がりを体系的・動的に捉えるHANフレームワークを提案した  \n",
    "(ii) HANを既存のNTMアーキテクチャ(Attention is all you need)に統合し複数のデータ・セットでベースラインを上回った  \n",
    "(iii) ソースとターゲットのHANsそれぞれから得られた文脈情報が相補的であることを示す各HAN構成の切除研究を行なった  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 手法の肝\n",
    "通常の内積・FFNNのアテンションでなく、Attention is all you needで提案されたMulti-Head Attentionを用いて階層的アテンションを構成している  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 実験\n",
    "提案手法であるHANsをエンコーダ/デコーダ両方に適用したモデルが、中英・独英のTEDTalks・Subtitles・NewsコーパスでtransformerモデルのBLEUスコアを上回る  \n",
    "また、名詞・代名詞の翻訳精度や語彙一貫性・翻訳結合性も同じモデルがtransformerを上回った  \n",
    "またある文の翻訳において、提案モデルは以前の数文からもっとも関連の強い文と、その文の中で最も関連がある単語を正しく重み付けできていることがわかる  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 改善\n",
    "アノテーションつきデータを用い対話の繋がりをモデル化することでより翻訳精度を上げることを試みる  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 次に読む論文\n",
    "Attention is all you need"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
