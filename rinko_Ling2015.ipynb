{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding Function in Form: Compositional Character Models for Open Vocabulary Word Representation\n",
    "\n",
    "井佐原研究室 情報知能工学課程 B4 棚橋優希"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 概要\n",
    "ある単語について、その単語を構成する文字からベクトルを生成するモデルの提案  \n",
    "各単語ごとに独立したベクトルを持つ従来のモデルと比較して、提案モデルは文字ごとのベクトルとモデルのハイパーパラメータのみを必要とする  \n",
    "この提案モデルは言語モデルと品詞タグ付けにおいてstate-of-the-artな結果を残した  \n",
    "従来のベースラインと比較して、特に形態的に高級な言語(トルコ語)において有用性が認められた"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 背景\n",
    "単語をベクトル化するにあたり、意味的・構文的に類似するようにベクトル空間に写像したい  \n",
    "既存の手法は各単語ごとに独立したベクトルを有するという仮定に基づく  \n",
    "しかし、このような仮定は形態的に高級な言語において問題となる  \n",
    "\n",
    "### 問題\n",
    "たとえば、トルコ語では単語の末尾に接尾辞を付与することにより意味が変わる  \n",
    "ex. evde → in the house / evden → from the house (ev: house)  \n",
    "だが、既存の手法ではevとevde,evdenはまったく別の独立したベクトルとして認識される  \n",
    "\n",
    "また、オンラインデータのようなオープンなドメインでは日々単語が増え続けるため、どれだけコーパスを広く取ってもどうしても辞書外単語(OOV)が発生しうる  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 解決手法：C2W\n",
    "形態素から単語表現を構成する先行研究があった  \n",
    "しかし、形態素は形態素解析器に依存してしまう  \n",
    "そこで、より原子的な単位として文字を入力として採用した双方向LSTMのモデルを考える  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 実験：言語モデル\n",
    "単語ルックアップテーブル→リカレントLSTM→単語予測のモデルを構築  \n",
    "単語ルックアップテーブルをC2Wに置き換えることで実験  \n",
    "何が有用か？  \n",
    "→ 通常、テスト時に表れなかった単語(OOV)は対処できない  \n",
    "一般的なアプローチとして、UNKトークンに置くといった手法が取られる  \n",
    "しかし、提案モデルは文字を参照するため、たとえ未知語であっても独立したベクトルを生成できる  \n",
    "→ Perplexityの改善  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### データセット\n",
    "英語・ポルトガル語・カタロニア語・ドイツ語・トルコ語について、言語モデルの性能を調べる  \n",
    "これらの言語について、Wikipediaの記事から無作為に抽出し、その結果得られた100万単語を訓練に使用した  \n",
    "開発とテストには更に抽出した2万語を用いた  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 実験設定\n",
    "単語の次元d = 50  \n",
    "加えて、C2Wでの文字次元$d_C$ = 50, current state(層の数？)$d_{CS}$ = 150\n",
    "100文ごとのミニバッチ勾配降下で学習  \n",
    "出力される語彙は頻度の高い上位5000単語のみを含むものとし、それ以外は全部OOVとしてUNKトークンに"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 実験結果\n",
    "perplexityが改善された  \n",
    "また、使用されたパラメータ数は大幅に減少  \n",
    "処理時間もそこまで変わらない  "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
