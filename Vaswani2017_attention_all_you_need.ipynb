{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention is all you need\n",
    "Vaswani et al."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## どんなの\n",
    "RNN・CNNを用いずAttention機構のみを用いたニューラル翻訳モデル(Transformer)の提案  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 先行研究との違い\n",
    "RNN・LSTMのような系列変換ニューラル翻訳モデルは並列化できないという制約(→計算量が系列長に比例して増加)  \n",
    "(LSTMのパラメータに関する)Factorization tricksや条件付き計算による計算速度の大幅な改善は達成されたが、系列変換モデルにおける逐次計算の制約は未だ解決されていない  \n",
    "アテンション機構は系列モデルや伝達モデルにおいて必要不可欠な部分となっており、入力系列や出力系列の距離に関係なく依存関係をモデル化することができるが、このアテンション機構は多くの場合RNN(seq2seq)と併用される  \n",
    "本研究では再帰を用いず、代わりに入力と出力間の依存関係をモデル化したアテンション機構に完全に依存する、Transformrerモデルを提案する  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 手法のキモ\n",
    "### アテンションは辞書オブジェクト\n",
    "(一般的なseq2seqにおける)アテンションの流れ：ターゲット側の隠れ状態とソース側の隠れ状態で類似度による重みを取り、それらの重みつき平均を取る→重みの強い要素を一つ選びとる  \n",
    "\n",
    "つまりアテンションは以下の要素から構成される辞書オブジェクトと解釈できる  \n",
    "- クエリ(query) → ターゲット側隠れ状態\n",
    "- キー(key) → 重みを取るときのソース側隠れ状態\n",
    "- バリュー(value) → 重み付き平均を取って選び出されたソース側隠れ状態\n",
    "\n",
    "キーとバリューにわざわざ分離する意味とは？  \n",
    "→ キーからバリューを予測することが難しい、非自明な変換をキーとバリュー間に定義することで高い表現力を得る(文脈や推論、文の潜在的な結合などRNN等のモデルで表現しにくい情報をキー・バリューの繋がりによって無理やり表現できるようにする？)  \n",
    "\n",
    "### Self-Attention\n",
    "ソース側をキー・バリュー、ターゲット側をクエリとするのに対して、同じ系列からクエリ、キー、バリューを設定するアテンションをSelf-Attentionという  \n",
    "提案モデルでは下位の隠れ状態から次の隠れ状態を生成する際に用いられる  \n",
    "畳み込み層が局所的な隠れ状態しか参照できないのに対して、Self-Attentionは下位層のすべての状態を参照できてつよい  \n",
    "\n",
    "### Multi-Head Attention\n",
    "提案モデルではScaled Dot-Product Attention(正規化内積アテンション)という内積が大きくなりすぎないようにスケール化されたアテンションを用いるが、  \n",
    "実際にアテンションの計算を行う際、隠れ状態をh個のヘッドに分割し並列化された計算が行われる(隠れ状態d=512、h=8のとき異なる8個のWでd=64に線形写像された上でアテンション計算が行われ、最終的に出力された8つの64次元隠れ状態が結合される)  \n",
    "また、アテンションの計算時クエリ(とキーとバリュー)は一度に複数渡されて、一気に計算されることにも留意  \n",
    "複数ヘッドによる処理は異なる位置の異なる表現部分空間からの情報を学習できる(？？？？？？)  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 実験\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 次に読むの\n",
    "Selfattentive residual decoder for neural machine translation\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
